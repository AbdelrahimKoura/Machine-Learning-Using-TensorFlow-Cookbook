{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the TensorFlow way of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "housing_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'\n",
    "path = tf.keras.utils.get_file(housing_url.split(\"/\")[-1], housing_url)\n",
    "\n",
    "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "data = pd.read_table(path, delim_whitespace=True, header=None, names=columns)\n",
    "\n",
    "np.random.seed(1)\n",
    "train = data.sample(frac=0.8).copy()\n",
    "y_train = train['MEDV']\n",
    "train.drop('MEDV', axis=1, inplace=True)\n",
    "\n",
    "test = data.loc[~data.index.isin(train.index)].copy()\n",
    "y_test = test['MEDV']\n",
    "test.drop('MEDV', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_feature_columns(data_df, categorical_cols, numeric_cols):\n",
    "    feature_columns = []\n",
    "    \n",
    "    for feature_name in numeric_cols:\n",
    "        feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
    "    \n",
    "    for feature_name in categorical_cols:\n",
    "        vocabulary = data_df[feature_name].unique()\n",
    "        feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n",
    "\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=256):\n",
    "    \n",
    "    def input_function():\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(1000)\n",
    "        ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "        return ds\n",
    "    \n",
    "    return input_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['CHAS', 'RAD']\n",
    "numeric_cols = ['CRIM', 'ZN', 'INDUS',  'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "feature_columns = define_feature_columns(data, categorical_cols, numeric_cols)\n",
    "\n",
    "train_input_fn = make_input_fn(train, y_train, num_epochs=1400)\n",
    "test_input_fn = make_input_fn(test, y_test, num_epochs=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './logs/LinearRegressor_1'\n",
    "linear_est = tf.estimator.LinearRegressor(feature_columns=feature_columns,\n",
    "                                          model_dir = output_dir,\n",
    "                                          config=tf.estimator.RunConfig().replace(save_summary_steps=100))\n",
    "linear_est.train(train_input_fn)\n",
    "result = linear_est.evaluate(test_input_fn)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./logs/LinearRegressor_1\n",
    "# http://localhost:6006/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactions(interactions_list, buckets=5):\n",
    "    interactions = list()\n",
    "    for (a, b) in interactions_list:\n",
    "        interactions.append(tf.feature_column.crossed_column([a, b], hash_bucket_size=buckets))\n",
    "    return interactions\n",
    "\n",
    "derived_feature_columns = create_interactions([['RM', 'LSTAT']])\n",
    "\n",
    "output_dir = './logs/LinearRegressor_2'\n",
    "linear_est = tf.estimator.LinearRegressor(feature_columns=feature_columns+derived_feature_columns,\n",
    "                                          model_dir = output_dir,\n",
    "                                          config=tf.estimator.RunConfig().replace(save_summary_steps=100))\n",
    "\n",
    "linear_est.train(train_input_fn)\n",
    "\n",
    "result = linear_est.evaluate(test_input_fn)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./logs/LinearRegressor_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicts_to_preds(pred_dicts):\n",
    "    return np.array([pred['predictions'] for pred in pred_dicts])\n",
    "\n",
    "preds = dicts_to_preds(linear_est.predict(test_input_fn))\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras to estimator version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_feature_columns_layers(data_df, categorical_cols, numeric_cols):\n",
    "    feature_columns = []\n",
    "    feature_layer_inputs = {}\n",
    "    \n",
    "    for feature_name in numeric_cols:\n",
    "        feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
    "        feature_layer_inputs[feature_name] = tf.keras.Input(shape=(1,), name=feature_name)\n",
    "        \n",
    "    for feature_name in categorical_cols:\n",
    "        vocabulary = data_df[feature_name].unique()\n",
    "        cat = tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary)\n",
    "        cat_one_hot = tf.feature_column.indicator_column(cat)\n",
    "        feature_columns.append(cat_one_hot)\n",
    "        feature_layer_inputs[feature_name] = tf.keras.Input(shape=(1,), name=feature_name, dtype=tf.int32)\n",
    "    \n",
    "    return feature_columns, feature_layer_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactions(interactions_list, buckets=5):\n",
    "    feature_columns = []\n",
    "    \n",
    "    for (a, b) in interactions_list:\n",
    "        crossed_feature = tf.feature_column.crossed_column([a, b], hash_bucket_size=buckets)\n",
    "        crossed_feature_one_hot = tf.feature_column.indicator_column(crossed_feature)\n",
    "        feature_columns.append(crossed_feature_one_hot)\n",
    "        \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_linreg(feature_columns, feature_layer_inputs, optimizer):\n",
    "\n",
    "    feature_layer = keras.layers.DenseFeatures(feature_columns)\n",
    "    feature_layer_outputs = feature_layer(feature_layer_inputs)\n",
    "    norm = keras.layers.BatchNormalization()(feature_layer_outputs)\n",
    "    outputs = keras.layers.Dense(1, kernel_initializer='normal', activation='linear')(norm)\n",
    "    \n",
    "    model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=outputs)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['CHAS', 'RAD']\n",
    "numeric_cols = ['CRIM', 'ZN', 'INDUS',  'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "feature_columns, feature_layer_inputs = define_feature_columns_layers(data, categorical_cols, numeric_cols)\n",
    "interactions_columns = create_interactions([['RM', 'LSTAT']])\n",
    "\n",
    "feature_columns += interactions_columns\n",
    "\n",
    "optimizer = keras.optimizers.Ftrl(learning_rate=0.02)\n",
    "model = create_linreg(feature_columns, feature_layer_inputs, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "def canned_keras(model, output_dir = None):\n",
    "    if output_dir is None:\n",
    "        model_dir = tempfile.mkdtemp()\n",
    "    else:\n",
    "        model_dir = output_dir\n",
    "    keras_estimator = tf.keras.estimator.model_to_estimator(\n",
    "        keras_model=model, \n",
    "        model_dir=model_dir,\n",
    "        config=tf.estimator.RunConfig().replace(save_summary_steps=100))\n",
    "    return keras_estimator\n",
    "\n",
    "estimator = canned_keras(model, output_dir='./logs/canned_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = make_input_fn(train, y_train, num_epochs=1400)\n",
    "test_input_fn = make_input_fn(test, y_test, num_epochs=1, shuffle=False)\n",
    "\n",
    "estimator.train(train_input_fn)\n",
    "result = estimator.evaluate(test_input_fn)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./logs/canned_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding coefficients in linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = estimator.get_variable_value('layer_with_weights-1/kernel/.ATTRIBUTES/VARIABLE_VALUE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(feature_columns):\n",
    "    labels = list()\n",
    "    for col in feature_columns:\n",
    "        col_config = col.get_config()\n",
    "        if 'key' in col_config:\n",
    "            labels.append(col_config['key'])\n",
    "        elif 'categorical_column' in col_config:\n",
    "            if col_config['categorical_column']['class_name']=='VocabularyListCategoricalColumn':\n",
    "                key = col_config['categorical_column']['config']['key']\n",
    "                for item in col_config['categorical_column']['config']['vocabulary_list']:\n",
    "                     labels.append(key+'_val='+str(item))\n",
    "            elif col_config['categorical_column']['class_name']=='CrossedColumn':\n",
    "                keys = col_config['categorical_column']['config']['keys']\n",
    "                for bucket in range(col_config['categorical_column']['config']['hash_bucket_size']):\n",
    "                    labels.append('x'.join(keys)+'_bkt_'+str(bucket))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = extract_labels(feature_columns)\n",
    "\n",
    "for label, weight in zip(labels, weights):\n",
    "    print(f\"{label:15s} : {weight[0]:+.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding loss functions in linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_linreg(feature_columns, feature_layer_inputs, optimizer, \n",
    "                  loss='mean_squared_error', metrics=['mean_absolute_error']):\n",
    "\n",
    "    feature_layer = keras.layers.DenseFeatures(feature_columns)\n",
    "    feature_layer_outputs = feature_layer(feature_layer_inputs)\n",
    "    norm = keras.layers.BatchNormalization()(feature_layer_outputs)\n",
    "    outputs = keras.layers.Dense(1, kernel_initializer='normal', activation='linear')(norm)\n",
    "    \n",
    "    model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=outputs)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['CHAS', 'RAD']\n",
    "numeric_cols = ['CRIM', 'ZN', 'INDUS',  'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "feature_columns, feature_layer_inputs = define_feature_columns_layers(data, categorical_cols, numeric_cols)\n",
    "interactions_columns = create_interactions([['RM', 'LSTAT']])\n",
    "\n",
    "feature_columns += interactions_columns\n",
    "\n",
    "optimizer = keras.optimizers.Ftrl(learning_rate=0.02)\n",
    "model = create_linreg(feature_columns, feature_layer_inputs, optimizer,\n",
    "                      loss='mean_absolute_error', \n",
    "                      metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "\n",
    "estimator = canned_keras(model, output_dir='./logs/canned_2')\n",
    "\n",
    "train_input_fn = make_input_fn(train, y_train, num_epochs=1400)\n",
    "test_input_fn = make_input_fn(test, y_test, num_epochs=1, shuffle=False)\n",
    "\n",
    "estimator.train(train_input_fn)\n",
    "result = estimator.evaluate(test_input_fn)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./logs/canned_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing lasso and ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ridge_linreg(feature_columns, feature_layer_inputs, optimizer, \n",
    "                        loss='mean_squared_error', metrics=['mean_absolute_error'],\n",
    "                        l2=0.01):\n",
    "    \n",
    "    regularizer = keras.regularizers.l2(l2)\n",
    "\n",
    "    feature_layer = keras.layers.DenseFeatures(feature_columns)\n",
    "    feature_layer_outputs = feature_layer(feature_layer_inputs)\n",
    "    norm = keras.layers.BatchNormalization()(feature_layer_outputs)\n",
    "    outputs = keras.layers.Dense(1, \n",
    "                                 kernel_initializer='normal', \n",
    "                                 kernel_regularizer = regularizer, \n",
    "                                 activation='linear')(norm)\n",
    "    \n",
    "    model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=outputs)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['CHAS', 'RAD']\n",
    "numeric_cols = ['CRIM', 'ZN', 'INDUS',  'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "feature_columns, feature_layer_inputs = define_feature_columns_layers(data, categorical_cols, numeric_cols)\n",
    "interactions_columns = create_interactions([['RM', 'LSTAT']])\n",
    "\n",
    "feature_columns += interactions_columns\n",
    "\n",
    "optimizer = keras.optimizers.Ftrl(learning_rate=0.02)\n",
    "model = create_ridge_linreg(feature_columns, feature_layer_inputs, optimizer,\n",
    "                      loss='mean_squared_error', \n",
    "                      metrics=['mean_absolute_error', 'mean_squared_error'],\n",
    "                           l2=0.01)\n",
    "\n",
    "estimator = canned_keras(model, output_dir='./logs/ridge')\n",
    "\n",
    "train_input_fn = make_input_fn(train, y_train, num_epochs=1400)\n",
    "test_input_fn = make_input_fn(test, y_test, num_epochs=1, shuffle=False)\n",
    "\n",
    "estimator.train(train_input_fn)\n",
    "result = estimator.evaluate(test_input_fn)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir './logs/ridge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lasso_linreg(feature_columns, feature_layer_inputs, optimizer, \n",
    "                        loss='mean_squared_error', metrics=['mean_absolute_error'],\n",
    "                        l1=0.001):\n",
    "    \n",
    "    regularizer = keras.regularizers.l1(l1)\n",
    "\n",
    "    feature_layer = keras.layers.DenseFeatures(feature_columns)\n",
    "    feature_layer_outputs = feature_layer(feature_layer_inputs)\n",
    "    norm = keras.layers.BatchNormalization()(feature_layer_outputs)\n",
    "    outputs = keras.layers.Dense(1, \n",
    "                                 kernel_initializer='normal', \n",
    "                                 kernel_regularizer = regularizer, \n",
    "                                 activation='linear')(norm)\n",
    "    \n",
    "    model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=outputs)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['CHAS', 'RAD']\n",
    "numeric_cols = ['CRIM', 'ZN', 'INDUS',  'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "feature_columns, feature_layer_inputs = define_feature_columns_layers(data, categorical_cols, numeric_cols)\n",
    "interactions_columns = create_interactions([['RM', 'LSTAT']])\n",
    "\n",
    "feature_columns += interactions_columns\n",
    "\n",
    "optimizer = keras.optimizers.Ftrl(learning_rate=0.02)\n",
    "model = create_lasso_linreg(feature_columns, feature_layer_inputs, optimizer,\n",
    "                      loss='mean_squared_error', \n",
    "                      metrics=['mean_absolute_error', 'mean_squared_error'],\n",
    "                           l1=0.001)\n",
    "\n",
    "estimator = canned_keras(model, output_dir='./logs/lasso')\n",
    "\n",
    "train_input_fn = make_input_fn(train, y_train, num_epochs=1400)\n",
    "test_input_fn = make_input_fn(test, y_test, num_epochs=1, shuffle=False)\n",
    "\n",
    "estimator.train(train_input_fn)\n",
    "result = estimator.evaluate(test_input_fn)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir './logs/lasso'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing elastic net regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_elasticnet_linreg(feature_columns, feature_layer_inputs, optimizer, \n",
    "                        loss='mean_squared_error', metrics=['mean_absolute_error'],\n",
    "                        l1=0.001, l2=0.01):\n",
    "    \n",
    "    regularizer = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
    "\n",
    "    feature_layer = keras.layers.DenseFeatures(feature_columns)\n",
    "    feature_layer_outputs = feature_layer(feature_layer_inputs)\n",
    "    norm = keras.layers.BatchNormalization()(feature_layer_outputs)\n",
    "    outputs = keras.layers.Dense(1, \n",
    "                                 kernel_initializer='normal', \n",
    "                                 kernel_regularizer = regularizer, \n",
    "                                 activation='linear')(norm)\n",
    "    \n",
    "    model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=outputs)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['CHAS', 'RAD']\n",
    "numeric_cols = ['CRIM', 'ZN', 'INDUS',  'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "feature_columns, feature_layer_inputs = define_feature_columns_layers(data, categorical_cols, numeric_cols)\n",
    "interactions_columns = create_interactions([['RM', 'LSTAT']])\n",
    "\n",
    "feature_columns += interactions_columns\n",
    "\n",
    "optimizer = keras.optimizers.Ftrl(learning_rate=0.02)\n",
    "model = create_elasticnet_linreg(feature_columns, feature_layer_inputs, optimizer,\n",
    "                      loss='mean_squared_error', \n",
    "                      metrics=['mean_absolute_error', 'mean_squared_error'],\n",
    "                           l1=0.001, l2=0.01)\n",
    "\n",
    "estimator = canned_keras(model, output_dir='./logs/elasticnet')\n",
    "\n",
    "train_input_fn = make_input_fn(train, y_train, num_epochs=1400)\n",
    "test_input_fn = make_input_fn(test, y_test, num_epochs=1, shuffle=False)\n",
    "\n",
    "estimator.train(train_input_fn)\n",
    "result = estimator.evaluate(test_input_fn)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir './logs/elasticnet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data'\n",
    "path = tf.keras.utils.get_file(breast_cancer.split(\"/\")[-1], breast_cancer)\n",
    "\n",
    "columns = ['sample_code', 'clump_thickness', 'cell_size_uniformity', 'cell_shape_uniformity',\n",
    "           'marginal_adhesion', 'single_epithelial_cell_size', 'bare_nuclei', 'bland_chromatin',\n",
    "           'normal_nucleoli', 'mitoses', 'class']\n",
    "\n",
    "data = pd.read_csv(path, header=None, names=columns, na_values=[np.nan, '?'])\n",
    "data = data.fillna(data.median())\n",
    "\n",
    "np.random.seed(1)\n",
    "train = data.sample(frac=0.8).copy()\n",
    "y_train = (train['class']==4).astype(int)\n",
    "train.drop(['sample_code', 'class'], axis=1, inplace=True)\n",
    "\n",
    "test = data.loc[~data.index.isin(train.index)].copy()\n",
    "y_test = (test['class']==4).astype(int)\n",
    "test.drop(['sample_code', 'class'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logreg(feature_columns, feature_layer_inputs, optimizer, \n",
    "                  loss='binary_crossentropy', metrics=['accuracy'],\n",
    "                  l2=0.01):\n",
    "    \n",
    "    regularizer = keras.regularizers.l2(l2)\n",
    "\n",
    "    feature_layer = keras.layers.DenseFeatures(feature_columns)\n",
    "    feature_layer_outputs = feature_layer(feature_layer_inputs)\n",
    "    norm = keras.layers.BatchNormalization()(feature_layer_outputs)\n",
    "    outputs = keras.layers.Dense(1, \n",
    "                                 kernel_initializer='normal', \n",
    "                                 kernel_regularizer = regularizer, \n",
    "                                 activation='sigmoid')(norm)\n",
    "    \n",
    "    model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=outputs)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = []\n",
    "numeric_cols = ['clump_thickness', 'cell_size_uniformity', 'cell_shape_uniformity',\n",
    "                'marginal_adhesion', 'single_epithelial_cell_size', 'bare_nuclei', 'bland_chromatin',\n",
    "                'normal_nucleoli', 'mitoses']\n",
    "\n",
    "feature_columns, feature_layer_inputs = define_feature_columns_layers(data, categorical_cols, numeric_cols)\n",
    "\n",
    "optimizer = keras.optimizers.Ftrl(learning_rate=0.007)\n",
    "model = create_logreg(feature_columns, feature_layer_inputs, optimizer, l2=0.01)\n",
    "\n",
    "estimator = canned_keras(model, output_dir='./logs/logreg')\n",
    "\n",
    "train_input_fn = make_input_fn(train, y_train, num_epochs=300, batch_size=32)\n",
    "test_input_fn = make_input_fn(test, y_test, num_epochs=1, shuffle=False)\n",
    "\n",
    "estimator.train(train_input_fn)\n",
    "result = estimator.evaluate(test_input_fn)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir './logs/logreg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resorting to non-linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tensorflow.python.keras.layers.kernelized import RandomFourierFeatures\n",
    "except:\n",
    "    # from TF 2.2\n",
    "    from tensorflow.keras.layers.experimental import RandomFourierFeatures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_svc(feature_columns, feature_layer_inputs, optimizer, \n",
    "               loss='hinge', metrics=['accuracy'],\n",
    "               l2=0.01, output_dim=64, scale=None):\n",
    "    \n",
    "    regularizer = keras.regularizers.l2(l2)\n",
    "\n",
    "    feature_layer = keras.layers.DenseFeatures(feature_columns)\n",
    "    feature_layer_outputs = feature_layer(feature_layer_inputs)\n",
    "    norm = keras.layers.BatchNormalization()(feature_layer_outputs)\n",
    "    rff = RandomFourierFeatures(output_dim=output_dim, scale=scale, kernel_initializer='gaussian')(norm)\n",
    "    outputs = keras.layers.Dense(1, \n",
    "                                 kernel_initializer='normal', \n",
    "                                 kernel_regularizer = regularizer, \n",
    "                                 activation='sigmoid')(rff)\n",
    "    \n",
    "    model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=outputs)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./log/rff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = []\n",
    "numeric_cols = ['clump_thickness', 'cell_size_uniformity', 'cell_shape_uniformity',\n",
    "                'marginal_adhesion', 'single_epithelial_cell_size', 'bare_nuclei', 'bland_chromatin',\n",
    "                'normal_nucleoli', 'mitoses']\n",
    "\n",
    "feature_columns, feature_layer_inputs = define_feature_columns_layers(data, categorical_cols, numeric_cols)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00005)\n",
    "model = create_svc(feature_columns, feature_layer_inputs, optimizer, \n",
    "                   loss='hinge', l2=0.001, output_dim=512)\n",
    "\n",
    "estimator = canned_keras(model, output_dir='./log/rff')\n",
    "\n",
    "train_input_fn = make_input_fn(train, y_train, num_epochs=500, batch_size=512)\n",
    "test_input_fn = make_input_fn(test, y_test, num_epochs=1, shuffle=False)\n",
    "\n",
    "estimator.train(train_input_fn)\n",
    "result = estimator.evaluate(test_input_fn)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir './log/rff'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Wide & Deep Learning for Recommender Systems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_dir = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/'\n",
    "train_path = tf.keras.utils.get_file('adult.data', census_dir + 'adult.data')\n",
    "test_path = tf.keras.utils.get_file('adult.test', census_dir + 'adult.test')\n",
    "\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "           'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "           'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "           'income_bracket']\n",
    "\n",
    "train_data = pd.read_csv(train_path, header=None, names=columns)\n",
    "test_data = pd.read_csv(test_path, header=None, names=columns, skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['age', 'workclass', 'education', 'education_num',\n",
    "              'marital_status', 'occupation', 'relationship', 'gender']\n",
    "\n",
    "y_train = (train_data.income_bracket==' >50K').astype(int)\n",
    "y_test = (test_data.income_bracket==' >50K.').astype(int)\n",
    "\n",
    "train_data = train_data[predictors]\n",
    "test_data = test_data[predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['age', 'education_num']] = train_data[['age', 'education_num']].fillna(train_data[['age', 'education_num']].mean())\n",
    "test_data[['age', 'education_num']] = test_data[['age', 'education_num']].fillna(train_data[['age', 'education_num']].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_feature_columns(data_df, numeric_cols, categorical_cols, categorical_embeds, dimension=30):\n",
    "    numeric_columns = []\n",
    "    categorical_columns = []\n",
    "    embeddings = []\n",
    "    \n",
    "    for feature_name in numeric_cols:\n",
    "        numeric_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
    "    \n",
    "    for feature_name in categorical_cols:\n",
    "        vocabulary = data_df[feature_name].unique()\n",
    "        categorical_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n",
    "        \n",
    "    for feature_name in categorical_embeds:\n",
    "        vocabulary = data_df[feature_name].unique()\n",
    "        to_categorical = tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary)\n",
    "        embeddings.append(tf.feature_column.embedding_column(to_categorical, dimension=dimension))\n",
    "        \n",
    "    return numeric_columns, categorical_columns, embeddings\n",
    "\n",
    "def create_interactions(interactions_list, buckets=10):\n",
    "    feature_columns = []\n",
    "    \n",
    "    for (a, b) in interactions_list:\n",
    "        crossed_feature = tf.feature_column.crossed_column([a, b], hash_bucket_size=buckets)\n",
    "        crossed_feature_one_hot = tf.feature_column.indicator_column(crossed_feature)\n",
    "        feature_columns.append(crossed_feature_one_hot)\n",
    "        \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns, categorical_columns, embeddings = define_feature_columns(train_data, \n",
    "                                                                          numeric_cols=['age', 'education_num'], \n",
    "                                                                          categorical_cols=['gender'], \n",
    "                                                                          categorical_embeds=['workclass', 'education',\n",
    "                                                                                              'marital_status', 'occupation', \n",
    "                                                                                              'relationship'], \n",
    "                                                                          dimension=32)\n",
    "\n",
    "interactions = create_interactions([['education', 'occupation']], buckets=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf './logs/wd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './logs/wd'\n",
    "\n",
    "estimator = tf.estimator.DNNLinearCombinedClassifier(\n",
    "    # wide settings\n",
    "    linear_feature_columns=numeric_columns+categorical_columns+interactions,\n",
    "    linear_optimizer=keras.optimizers.Ftrl(learning_rate=0.0002),\n",
    "    # deep settings\n",
    "    dnn_feature_columns=embeddings,\n",
    "    dnn_hidden_units=[1024, 256, 128, 64],\n",
    "    dnn_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    model_dir = output_dir,\n",
    "    config=tf.estimator.RunConfig().replace(save_summary_steps=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=256):\n",
    "    \n",
    "    def input_function():\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(1000)\n",
    "        ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "        return ds\n",
    "    \n",
    "    return input_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = make_input_fn(train_data, y_train, num_epochs=100, batch_size=256)\n",
    "test_input_fn = make_input_fn(test_data, y_test, num_epochs=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=train_input_fn, steps=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = estimator.evaluate(input_fn=test_input_fn)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(predictor):\n",
    "    preds = list()\n",
    "    for pred in predictor:\n",
    "        preds.append(pred['probabilities'])\n",
    "    return np.array(preds)\n",
    "http://localhost:8888/notebooks/4.%20Linear%20Regression%20with%20TensorBoard.ipynb#\n",
    "predictions = predict_proba(estimator.predict(input_fn=test_input_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./logs/wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
